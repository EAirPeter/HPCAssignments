\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{diagbox}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{rotating}
\usepackage{fontspec}
\usepackage{filecontents}
\usepackage{pgfplots}

\setmonofont{Consolas}

\title{Report for HPC Homework 5}
\author{Zhen Li}
\date{\today}

\begin{filecontents*}{weak.csv}
NP,BL,NB
1,0.114321,0.120982
4,0.195516,0.17968
16,0.23157,0.210041
64,1.154025,0.991806
256,15.336503,13.124876
\end{filecontents*}

\begin{filecontents*}{strong.csv}
NP,BL,NB
1,1,1
4,4.065592752,3.544269397
16,9.497281424,8.88043386
64,34.48270979,31.86768355
256,93.30870841,32.82200698
\end{filecontents*}

\begin{document}

\maketitle

\begin{enumerate}
  \item MPI-parallel two-dimensional Jacobi smoother
  \begin{itemize}
    \item Every points reside in the $N_l \times N_l$ matrix should be updated
    by current process.

    \item The leftmost column, rightmost column, topmost row and the
    bottommost row should be communicated to the adjacent left, right, up,
    down process.

    \item See \url{jacobi.cpp} for blocking implementation and
    \url{jacobi-nb.cpp} for non-blocking implementation.

    \item See \url{jobs/weak/} and \url{jobs/strong/} for job scripts and raw
    results generated on Prince cluster. In the file names, \lstinline{bl}
    stands for blocking version, \lstinline{nb} stands for non-blocking
    version, \lstinline{n} stands for $N$, \lstinline{ln} stands for $N_l$ and
    \lstinline{np} stands for number of processes in the file names.

    \item For convenience, results are shown in the following table:
    \begin{center}
      \begin{tabular}{cccccc}
        \toprule
        $N$ & $N_l$ & \#Iteration & \#Process & Time (blocking) &
          Time (non-blocking) \\
        \midrule
        100   & 100   & 10000 & 1   & 0.114321   s & 0.120982   s \\
        200   & 100   & 10000 & 4   & 0.195516   s & 0.17968    s \\
        400   & 100   & 10000 & 16  & 0.23157    s & 0.210041   s \\
        800   & 100   & 10000 & 64  & 1.154025   s & 0.991806   s \\
        1600  & 100   & 10000 & 256 & 15.336503  s & 13.124876  s \\
        \midrule
        25600 & 25600 & 100   & 1   & 163.796906 s & 152.059074 s \\
        25600 & 12800 & 100   & 4   & 40.288567  s & 42.902798  s \\
        25600 & 6400  & 100   & 16  & 17.246715  s & 17.122933  s \\
        25600 & 3200  & 100   & 64  & 4.750117   s & 4.771576   s \\
        25600 & 1600  & 100   & 256 & 1.75543    s & 4.632839   s \\
        \bottomrule
      \end{tabular}
    \end{center}
    where the upper half is for weak scalability comparison and the lower half
    is for strong scalability comparison.

    \item For weak scalability, the timings are plotted below. As you may see,
    this algorithm is not weakly scalable since the running time significantly
    increases as $N$ and \#Process grows.
    \begin{center} \begin{tikzpicture}
      \begin{axis} [
        xlabel = {\#Process},
        ylabel = {Time (sec)},
        scale only axis,
        axis x line* = bottom,
        axis y line* = left,
        legend style = {
          at = {(0.1, 0.9)},
          anchor=north west,
        },
      ]
        \addplot table [x=NP, y=BL, col sep=comma]{weak.csv};
        \addplot table [x=NP, y=NB, col sep=comma]{weak.csv};
        \legend{Blocking, Non-blocking}
      \end{axis}
    \end{tikzpicture} \end{center}

    \item For strong scalability, the speedups are plotted below. The series
    of blocking version some how resembles linear speedup, but it is still not
    ideal. For the non-blocking version, the running time of \#Process=256 is
    almost the same with that of \#Process=64, which might be caused by
    instability of the system. I have to mention that a previous buggy test of
    non-blocking version with \#Process=256 does suggest the running time is
    within 2 sec, which is what I expect. Since the nodes required for
    \#Process=256 are down now, I am not able to run the test again.
    \begin{center} \begin{tikzpicture}
      \begin{axis} [
        xlabel = {\#Process},
        ylabel = {Speedup},
        scale only axis,
        axis x line* = bottom,
        axis y line* = left,
        legend style = {
          at = {(0.1, 0.9)},
          anchor=north west,
        },
      ]
        \addplot table [x=NP, y=BL, col sep=comma]{strong.csv};
        \addplot table [x=NP, y=NB, col sep=comma]{strong.csv};
        \addplot table [x=NP, y=NP, col sep=comma]{strong.csv};
        \legend{Blocking, Non-blocking, Ideal}
      \end{axis}
    \end{tikzpicture} \end{center}

    \item For comparison between blocking and non-blocking version, actually
    their timings do not differ much but the non-blocking version still
    outperforms the blocking one when \#Process and $N$ both getting bigger.
  \end{itemize}

  \item Parallel sample sort
  \begin{itemize}
    \item See \url{ssort.cpp} for implementation.
    \item See \url{jobs/ssort/} for job scripts and raw results.
    \item The results are listed below. They are obtained with the
    configuration of \#Node=12 and \#TaskPerNode=14, that is \#Process=168.
    \begin{center}
      \begin{tabular}{ccc}
        \toprule
        $N$ & $P$ & Time \\
        \midrule
        10000   & 168 & 0.068062  s \\
        100000  & 168 & 0.718512  s \\
        1000000 & 168 & 20.496098 s \\
        \bottomrule
      \end{tabular}
    \end{center}
    \item You may notice that for $N=1000000$ the running time increases a
    a lot compared to smaller $N$'s. I guess it is also because of instability
    of the system.
    \item Note: the output of all bucket is removed from the repository
    because they are too large (about 2 gigabytes). You may generate them by
    running the program.
  \end{itemize}
\end{enumerate}

\end{document}
